\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{cite}

\title{Project 2\\Using a Feed-Forward Neural Network with a Sigmoid Activation Function and Supervised Learning Techniques to Predict Tulsa Weather}

\author{Christian Mann, Steven Reed \\
		University of Tulsa}

\begin{document}

\maketitle

\begin{multicols}{2}

\begin{abstract}
In this paper we develop our own Feed-Forward Neural Network based off of previous examples found in the book, Fundamentals of Computational Neuroscience \cite{trappenberg}, and on the Internet. Our Neural Network features Backpropogation for supervised learning. We then use it to approximate various parametric functions. Finally we attempt to approximate the behavior of weather in Tulsa, OK based on past weather data provided by the NOAA.
\end{abstract}

\section{Introduction}

A Neural Network can take many forms, but the ideas behind them all are derived from studies in the field of neuroscience. The general concept behind the structure of a Neural Network is simple to understand, a simple graph structure coupled with a few additional rules. Yet it proves to be an incredibly difficult challenge to create a network of any use. The purpose of a Neural Network is, in short, to approximate a function. Sometimes this function is known explicitly, but is difficult or expensive to compute. Other times the function cannot be explicitly known, but we can gather empirical data relating to the function and use it to create a network. Ultimately, the goal of a Neural Network is to approximate a function using whatever information about the function that we can get our hands on.

There are two main techniques in developing the structure of a Neural Network. The first technique is called Supervised Learning where the Network is ``trained'' by means of giving it example inputs and outputs from the function we want it to approximate. After pouring over the examples over and over, the Network (hopefully) converges to a certain structure and becomes well equipped at mimicking the original function. It does this by making many small changes to the weights associated with the edges in the network using a method known as ``backpropogation'' \cite{trappenberg}. The second popular method for constructing a Neural Network is known as Unsupervised Learning where the desired outcome is not known and the machine is responsible for finding correlations. We do not use any Unsupervised Learning techniques in this project.

In this project we wrote our own Neural Network in C++. We then used it to approximate various functions. These functions included the XOR function, a parametric circle, a parametric coffee cup, and ultimately using Tulsa, OK weather data to predict future weather in Tulsa, OK.

\section{Building a Neural Network}

We began by researching existing Neural Networks to get an idea for what kind of elements should be present in a network. There are many already availabe, and all have a varying set of features and capabilities. Many implementations are able to take advantage of a computer's GPU to parallelize the computations. The most simplistic example Network we could find, though, was a Javascript implementation known as Brain.js \cite{brain}. We used Brain.js as a guide for creating our own Neural Network in C++ with the hopes of performing faster than Javascript at a lower level, but not near the performance of a GPGPU solution.

We chose to use the Sigmoid function, $S(t)$, for its simplicity and convenient derivative. The convenient derivative is important as it will be computed a lot during training. Using a non-linear activation function also keeps our network from being limited to approximating linear functions \cite{hornik}. The derivative is used to modify error terms for each node which are eventually used to effect the change in weight to incoming edges. Since the derivate of the sigmoid function is highest for values around 0 (and S(0) = 0.5), multiplying the error by the derivative tends to favors error associated with outputs where $S(output)\approx0.5$, in turn causing them to adjust weights quicker.

	\[S(t) = \frac{1}{1+e^{-t}}\]
	\[\frac{d}{dt}S(t) = S(t) * (1-S(t))\]

In addition to the use of the Sigmoid function for node activation, and per-node bias, we discovered the use of a momentum factor in Brain.js which we also incorporated in our own model. The momentum is intended to help the network avoid local minima by increasing any change made to a given edge during training by some fraction of the change made to the edge on the previous iteration. It is intutive to think of this as momentum; after an edge has a large change applied to it, it will take several iterations to 

The primary initial parameters that must be set prior to running our network are learning rate, momentum, hidden layer dimensions, error threshold, and maximum iterations. The learning rate constant can be thought of as a magnitude adjustment for every change that occurs to an edge during training. The momentum constant determines how much change is retained and reapplied to the next change. The hidden layer dimensions determine how many hidden layers there are and how many nodes are in each layer. The layers do not need to contain the same number of nodes by any means. Lastly the error threshold and the maximum iterations constants are used as stopping conditions. Once either threshold has been met, the network will consider training finished. At this point, the network can be used just like the original function.

\section{Approximating the XOR Function}

Once the network was approaching completion, the first function we used to begin testing the network was the simple, yet popular, XOR function. After some debugging and fine tuning, we were able to build a network of 5 nodes (2 input, 1 output, and 1 hidden layer with 2 nodes) which was trained to approximate XOR. The only training patterns given were the 4 boolean expressions in Table~\ref{tab:xortable}, but each one was used to train the network roughly 4000 times to achieve adequate results with a sufficiently low error. The results of the inputs being fed through the resulting trained network can be seen in the right hand column of Table~\ref{tab:xortable}.

However, we were not satisfied to know that XOR performed appropriately for exactly the inputs we fed it repeatedly, we were curious what the rest of the domain looked like after training. For example, what did $(I_1=0.34, I_2=0.64)$ look like? To see what the rest of the graph looked like, we fed the trained network a circle of $X$ and $Y$ inputs. We were suprised to see the gradual rise and fall of the 3D surface shown in Figure~\ref{fig:xor1} and Figure~\ref{fig:xor2}.

\section{Approximating a Parametric Circle}

Once we had the XOR function working properly, we wanted to step up the difficulty. Next we wanted to approximated a parametric function with one input as the $t$ value and two outputs for the $x$ and $y$ values. We created roughly 30 data points approximating the parametric plot of a circle, and trained the network for many iterations longer than the XOR function. We then used a validation set of 100 datapoints around the circle to see how the network performed. The result can be seen in Figure~\ref{fig:circle} where the red circle is the actual validation set, and the blue circle is the one approximated by the network.

\section{Approximating a Parametric Coffee Cup}

As a more difficult challenge in parametric plot approximations, we used Wolfram Alpha to generate a parametric plot drawing of a coffee cup \cite{wolfram}. The training data of the original plot can be seen in Figure~\ref{fig:coffee1}. Various neural network dimensions were used to try to approximate this coffee cup, but in the end it proved far too complex to do in a small amount of time. After training the neural network for 3 Million iterations, the graph in Figure~\ref{fig:coffee4} was achieved. The red outline is the training data while the blue outline is the output of the neural network. It's clear that the training had some significant effect, but did not converge as well as we had hoped. Some shapes are clearly defined, such as the outline of the steam on the right. Other sections, such as the handle and the body, make it clear what the network was going for, but still look like the drawing of a 3 year old. However, the right hand side of the saucer plate is a complete mess.

\section{Weather Prediction}

\section{Conclusion and Future Work}

It is clear that our network approximates functions to some extent, however, it is also clear that it could benefit from more features. Trying other non-linear activation functions than our chosen Sigmoid would be one way to generate different, and perhaps more accurate, output. Another way would be to try various parameter settings for learning rate, momentum, hidden layer dimensions, and training/validation sets.

\bibliographystyle{abbrv}
\bibliography{citations}

\end{multicols}

\begin{table}[ht]
	\caption{XOR Truth Table}
	\label{tab:xortable}
	\begin{center}
		\begin{tabular}{cc|cc}
		\hline
		\hline
		\textbf{$In_1$} & \textbf{$In_2$} & \textbf{$Out$} & NeuralNet \\
		\hline
			$0.0$ & $0.0$ & $0.0$ & $0.0062$\\
			$0.0$ & $1.0$ & $1.0$ & $0.9934$\\
			$1.0$ & $0.0$ & $1.0$ & $0.9934$\\
			$1.0$ & $1.0$ & $0.0$ & $0.0086$\\
		\hline

		\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[scale=0.5]{img/xor1}
	\end{center}
	\caption{XOR function}
	\label{fig:xor1}
\end{figure}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[scale=0.5]{img/xor2}
	\end{center}
	\caption{XOR Function}
	\label{fig:xor2}
\end{figure}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[scale=0.5]{img/circle}
	\end{center}
	\caption{Parametric Circle Function}
	\label{fig:circle}
\end{figure}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[scale=0.45]{img/coffee1}
	\end{center}
	\caption{Parametric Coffee Function}
	\label{fig:coffee1}
\end{figure}

\begin{figure}[tb]
	\begin{center}
		\includegraphics[scale=0.4]{img/coffee4}
	\end{center}
	\caption{Neural Coffee Approximation in Blue}
	\label{fig:coffee4}
\end{figure}

\end{document}

% citations:
% weather data